{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验内容：  \n",
    "使用LendingClub Safe Loans数据集：\n",
    "1. 实现信息增益、信息增益率、基尼指数三种划分标准\n",
    "2. 使用给定的训练集完成三种决策树的训练过程\n",
    "3. 计算三种决策树在最大深度为6时测试集上的精度，查准率，查全率，F1值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这部分，我们会实现一个很简单的二叉决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入类库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "loans = pd.read_csv('data/lendingclub/lending-club-data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据中有两列是我们想预测的指标，一项是safe_loans，一项是bad_loans，分别表示正例和负例，我们对其进行处理，将正例的safe_loans设为1，负例设为-1，删除bad_loans这列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据进行预处理，将safe_loans作为标记\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "del loans['bad_loans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们只使用grade, term, home_ownership, emp_length这四列作为特征，safe_loans作为标记，只保留loans中的这五列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看前五行数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>term</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>safe_loans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>60 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>3 years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  grade        term home_ownership emp_length  safe_loans\n",
       "0     B   36 months           RENT  10+ years           1\n",
       "1     C   60 months           RENT   < 1 year          -1\n",
       "2     C   36 months           RENT  10+ years           1\n",
       "3     C   36 months           RENT  10+ years           1\n",
       "4     A   36 months           RENT    3 years           1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "loans = shuffle(loans, random_state = 34)\n",
    "\n",
    "split_line = int(len(loans) * 0.6)\n",
    "train_data = loans.iloc[: split_line]\n",
    "test_data = loans.iloc[split_line:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到所有的特征都是离散类型的特征，需要对数据进行预处理，使用one-hot编码对其进行处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot编码的思想就是将离散特征变成向量，假设特征$A$有三种取值$\\{a, b, c\\}$，这三种取值等价，如果我们使用1,2,3三个数字表示这三种取值，那么在计算时就会产生偏差，有一些涉及距离度量的算法会认为，2和1离得近，3和1离得远，但这三个值应该是等价的，这种表示方法会造成模型在判断上出现偏差。解决方案就是使用一个三维向量表示他们，用$[1, 0, 0]$表示a，$[0, 1, 0]$表示b，$[0, 0, 1]$表示c，这样三个向量之间的距离就都是相等的了，任意两个向量在欧式空间的距离都是$\\sqrt{2}$。这就是one-hot编码是思想。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas中使用get_dummies生成one-hot向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data, features_categorical):\n",
    "    '''\n",
    "    Parameter\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "    \n",
    "    features_categorical: list(str)\n",
    "    '''\n",
    "    \n",
    "    # 对所有的离散特征遍历\n",
    "    for cat in features_categorical:\n",
    "        \n",
    "        # 对这列进行one-hot编码，前缀为这个变量名\n",
    "        one_encoding = pd.get_dummies(data[cat], prefix = cat)\n",
    "        \n",
    "        # 将生成的one-hot编码与之前的dataframe拼接起来\n",
    "        data = pd.concat([data, one_encoding],axis=1)\n",
    "        \n",
    "        # 删除掉原始的这列离散特征\n",
    "        del data[cat]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先对训练集生成one-hot向量，然后对测试集生成one-hot向量，这里需要注意的是，如果训练集中，特征$A$的取值为$\\{a, b, c\\}$，这样我们生成的特征就有三列，分别为$A\\_a$, $A\\_b$, $A\\_c$，然后我们使用这个训练集训练模型，模型就就会考虑这三个特征，在测试集中如果有一个样本的特征$A$的值为$d$，那它的$A\\_a$，$A\\_b$，$A\\_c$就都为0，我们不去考虑$A\\_d$，因为这个特征在训练模型的时候是不存在的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = one_hot_encoding(train_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_loans</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>term_ 36 months</th>\n",
       "      <th>term_ 60 months</th>\n",
       "      <th>...</th>\n",
       "      <th>emp_length_10+ years</th>\n",
       "      <th>emp_length_2 years</th>\n",
       "      <th>emp_length_3 years</th>\n",
       "      <th>emp_length_4 years</th>\n",
       "      <th>emp_length_5 years</th>\n",
       "      <th>emp_length_6 years</th>\n",
       "      <th>emp_length_7 years</th>\n",
       "      <th>emp_length_8 years</th>\n",
       "      <th>emp_length_9 years</th>\n",
       "      <th>emp_length_&lt; 1 year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84320</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121308</th>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58376</th>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66430</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65344</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        safe_loans  grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  \\\n",
       "84320            1    False     True    False    False    False    False   \n",
       "121308          -1     True    False    False    False    False    False   \n",
       "58376           -1    False    False    False     True    False    False   \n",
       "66430            1     True    False    False    False    False    False   \n",
       "65344            1    False    False     True    False    False    False   \n",
       "\n",
       "        grade_G  term_ 36 months  term_ 60 months  ...  emp_length_10+ years  \\\n",
       "84320     False             True            False  ...                 False   \n",
       "121308    False             True            False  ...                 False   \n",
       "58376     False             True            False  ...                 False   \n",
       "66430     False            False             True  ...                 False   \n",
       "65344     False             True            False  ...                 False   \n",
       "\n",
       "        emp_length_2 years  emp_length_3 years  emp_length_4 years  \\\n",
       "84320                False               False               False   \n",
       "121308               False               False                True   \n",
       "58376                False               False               False   \n",
       "66430                 True               False               False   \n",
       "65344                False               False               False   \n",
       "\n",
       "        emp_length_5 years  emp_length_6 years  emp_length_7 years  \\\n",
       "84320                False               False                True   \n",
       "121308               False               False               False   \n",
       "58376                False               False               False   \n",
       "66430                False               False               False   \n",
       "65344                False               False               False   \n",
       "\n",
       "        emp_length_8 years  emp_length_9 years  emp_length_< 1 year  \n",
       "84320                False               False                False  \n",
       "121308               False               False                False  \n",
       "58376                False               False                 True  \n",
       "66430                False               False                False  \n",
       "65344                False               False                False  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取所有特征的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'emp_length_1 year',\n",
       " 'emp_length_10+ years',\n",
       " 'emp_length_2 years',\n",
       " 'emp_length_3 years',\n",
       " 'emp_length_4 years',\n",
       " 'emp_length_5 years',\n",
       " 'emp_length_6 years',\n",
       " 'emp_length_7 years',\n",
       " 'emp_length_8 years',\n",
       " 'emp_length_9 years',\n",
       " 'emp_length_< 1 year']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_features = train_data.columns.tolist()\n",
    "one_hot_features.remove(target)\n",
    "one_hot_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是对测试集进行one_hot编码，但只要保留出现在one_hot_features中的特征即可·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tmp = one_hot_encoding(test_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个空的DataFrame\n",
    "test_data = pd.DataFrame(columns = train_data.columns)\n",
    "for feature in train_data.columns:\n",
    "    # 如果训练集中当前特征在test_data_tmp中出现了，将其复制到test_data中\n",
    "    if feature in test_data_tmp.columns:\n",
    "        test_data[feature] = test_data_tmp[feature].copy()\n",
    "    else:\n",
    "        # 否则就用全为0的列去替代\n",
    "        test_data[feature] = np.zeros(test_data_tmp.shape[0], dtype = 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_loans</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>term_ 36 months</th>\n",
       "      <th>term_ 60 months</th>\n",
       "      <th>...</th>\n",
       "      <th>emp_length_10+ years</th>\n",
       "      <th>emp_length_2 years</th>\n",
       "      <th>emp_length_3 years</th>\n",
       "      <th>emp_length_4 years</th>\n",
       "      <th>emp_length_5 years</th>\n",
       "      <th>emp_length_6 years</th>\n",
       "      <th>emp_length_7 years</th>\n",
       "      <th>emp_length_8 years</th>\n",
       "      <th>emp_length_9 years</th>\n",
       "      <th>emp_length_&lt; 1 year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37225</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101585</th>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31865</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97692</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88181</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        safe_loans  grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  \\\n",
       "37225            1    False    False    False    False     True    False   \n",
       "101585          -1    False     True    False    False    False    False   \n",
       "31865            1     True    False    False    False    False    False   \n",
       "97692            1    False    False     True    False    False    False   \n",
       "88181            1     True    False    False    False    False    False   \n",
       "\n",
       "        grade_G  term_ 36 months  term_ 60 months  ...  emp_length_10+ years  \\\n",
       "37225     False             True            False  ...                 False   \n",
       "101585    False             True            False  ...                 False   \n",
       "31865     False             True            False  ...                 False   \n",
       "97692     False             True            False  ...                  True   \n",
       "88181     False             True            False  ...                 False   \n",
       "\n",
       "        emp_length_2 years  emp_length_3 years  emp_length_4 years  \\\n",
       "37225                 True               False               False   \n",
       "101585               False               False                True   \n",
       "31865                False               False               False   \n",
       "97692                False               False               False   \n",
       "88181                False               False               False   \n",
       "\n",
       "        emp_length_5 years  emp_length_6 years  emp_length_7 years  \\\n",
       "37225                False               False               False   \n",
       "101585               False               False               False   \n",
       "31865                False               False               False   \n",
       "97692                False               False               False   \n",
       "88181                False                True               False   \n",
       "\n",
       "        emp_length_8 years  emp_length_9 years  emp_length_< 1 year  \n",
       "37225                False               False                False  \n",
       "101585               False               False                False  \n",
       "31865                False               False                False  \n",
       "97692                False               False                False  \n",
       "88181                False               False                False  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73564, 25)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49043, 25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**处理完后，所有的特征都是0和1，标记是1和-1**，以上就是数据预处理流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 实现3种特征划分准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树中有很多常用的特征划分方法，比如信息增益、信息增益率、基尼指数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要实现一个函数，它的作用是，给定决策树的某个结点内的所有样本的标记，让它计算出对应划分指标的值是多少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们会实现上述三种划分指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里我们约定，将所有特征取值为0的样本，划分到左子树，特征取值为1的样本，划分到右子树**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 信息增益\n",
    "信息熵：\n",
    "$$\n",
    "\\mathrm{Ent}(D) = - \\sum^{\\vert \\mathcal{Y} \\vert}_{k = 1} p_k \\mathrm{log}_2 p_k\n",
    "$$\n",
    "\n",
    "信息增益：\n",
    "$$\n",
    "\\mathrm{Gain}(D, a) = \\mathrm{Ent}(D) - \\sum^{V}_{v=1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\mathrm{Ent}(D^v)\n",
    "$$\n",
    "\n",
    "计算信息熵时约定：若$p = 0$，则$p \\log_2p = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面的函数需要填写两个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_entropy(labels_in_node):\n",
    "    '''\n",
    "    求当前结点的信息熵\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    labels_in_node: np.ndarray, 如[-1, 1, -1, 1, 1]\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    float: information entropy\n",
    "    '''\n",
    "    # 统计样本总个数\n",
    "    num_of_samples = labels_in_node.shape[0]\n",
    "    \n",
    "    if num_of_samples == 0:\n",
    "        return 0\n",
    "    \n",
    "    # 统计出标记为1的个数\n",
    "    num_of_positive = len(labels_in_node[labels_in_node == 1])\n",
    "    \n",
    "    # 统计出标记为-1的个数\n",
    "    num_of_negative = len(labels_in_node[labels_in_node == -1])             # YOUR CODE HERE\n",
    "    \n",
    "    # 统计正例的概率\n",
    "    prob_positive = num_of_positive / num_of_samples\n",
    "    \n",
    "    # 统计负例的概率\n",
    "    prob_negative = num_of_negative / num_of_samples               # YOUR CODE HERE\n",
    "    \n",
    "    if prob_positive == 0:\n",
    "        positive_part = 0\n",
    "    else:\n",
    "        positive_part = prob_positive * np.log2(prob_positive)\n",
    "    \n",
    "    if prob_negative == 0:\n",
    "        negative_part = 0\n",
    "    else:\n",
    "        negative_part = prob_negative * np.log2(prob_negative)\n",
    "    \n",
    "    return - ( positive_part + negative_part )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是6个测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n",
      "0.863120568566631\n",
      "0.863120568566631\n",
      "0.9975025463691153\n",
      "-0.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 信息熵测试样例1\n",
    "example_labels = np.array([-1, -1, 1, 1, 1])\n",
    "print(information_entropy(example_labels)) # 0.97095\n",
    "\n",
    "# 信息熵测试样例2\n",
    "example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])\n",
    "print(information_entropy(example_labels)) # 0.86312\n",
    "    \n",
    "# 信息熵测试样例3\n",
    "example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])\n",
    "print(information_entropy(example_labels)) # 0.86312\n",
    "\n",
    "# 信息熵测试样例4\n",
    "example_labels = np.array([-1] * 9 + [1] * 8)\n",
    "print(information_entropy(example_labels)) # 0.99750\n",
    "\n",
    "# 信息熵测试样例5\n",
    "example_labels = np.array([1] * 8)\n",
    "print(information_entropy(example_labels)) # 0\n",
    "\n",
    "# 信息熵测试样例6\n",
    "example_labels = np.array([])\n",
    "print(information_entropy(example_labels)) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来完成计算所有特征的信息增益的函数  \n",
    "**需要填写三个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gains(data, features, target, annotate = False):\n",
    "    '''\n",
    "    计算所有特征的信息增益\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "        data: pd.DataFrame，传入的样本，带有特征和标记的dataframe\n",
    "        \n",
    "        features: list(str)，特征名组成的list\n",
    "        \n",
    "        target: str, 标记(label)的名字\n",
    "        \n",
    "        annotate, boolean，是否打印所有特征的信息增益值，默认为False\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "        information_gains: dict, key: str, 特征名\n",
    "                                 value: float，信息增益\n",
    "    '''\n",
    "    \n",
    "    # 我们将每个特征划分的信息增益值存储在一个dict中\n",
    "    # 键是特征名，值是信息增益值\n",
    "    information_gains = dict()\n",
    "    \n",
    "    # 对所有的特征进行遍历，使用信息增益对每个特征进行计算\n",
    "    for feature in features:\n",
    "        \n",
    "        # 左子树保证所有的样本的这个特征取值为0\n",
    "        left_split_target = data[data[feature] == 0][target]\n",
    "        \n",
    "        # 右子树保证所有的样本的这个特征取值为1\n",
    "        right_split_target =  data[data[feature] == 1][target]\n",
    "            \n",
    "        # 计算左子树的信息熵\n",
    "        left_entropy = information_entropy(left_split_target)\n",
    "        \n",
    "        # 计算左子树的权重\n",
    "        left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target))\n",
    "\n",
    "        # 计算右子树的信息熵\n",
    "        right_entropy = information_entropy(right_split_target)                         # YOUR CODE HERE\n",
    "        \n",
    "        # 计算右子树的权重\n",
    "        right_weight = len(right_split_target) / (len(left_split_target) + len(right_split_target))                  \n",
    "        \n",
    "        # 计算当前结点的信息熵\n",
    "        current_entropy = information_entropy(data[target])\n",
    "            \n",
    "        # 计算使用当前特征划分的信息增益\n",
    "        gain = current_entropy - (left_weight * left_entropy + right_weight * right_entropy)                 # YOUR CODE HERE\n",
    "        \n",
    "        # 将特征名与增益值以键值对的形式存储在information_gains中\n",
    "        information_gains[feature] = gain\n",
    "        \n",
    "        if annotate:\n",
    "            print(\" \", feature, gain)\n",
    "            \n",
    "    return information_gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017591980178887745\n",
      "0.014291850329400169\n",
      "0.0037049200345330435\n"
     ]
    }
   ],
   "source": [
    "# 信息增益测试样例1\n",
    "print(compute_information_gains(train_data, one_hot_features, target)['grade_A']) # 0.01759\n",
    "\n",
    "# 信息增益测试样例2\n",
    "print(compute_information_gains(train_data, one_hot_features, target)['term_ 60 months']) # 0.01429\n",
    "\n",
    "# 信息增益测试样例3\n",
    "print(compute_information_gains(train_data, one_hot_features, target)['grade_B']) # 0.00370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 信息增益率\n",
    "信息增益率：\n",
    "\n",
    "$$\n",
    "\\mathrm{Gain\\_ratio}(D, a) = \\frac{\\mathrm{Gain}(D, a)}{\\mathrm{IV}(a)}\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "\\mathrm{IV}(a) = - \\sum^V_{v=1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\log_2 \\frac{\\vert D^v \\vert}{\\vert D \\vert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成计算所有特征信息增益率的函数  \n",
    "**这里要完成四个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain_ratios(data, features, target, annotate = False):\n",
    "    '''\n",
    "    计算所有特征的信息增益率并保存起来\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    data: pd.DataFrame, 带有特征和标记的数据\n",
    "    \n",
    "    features: list(str)，特征名组成的list\n",
    "    \n",
    "    target: str， 特征的名字\n",
    "    \n",
    "    annotate: boolean, default False，是否打印注释\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    gain_ratios: dict, key: str, 特征名\n",
    "                       value: float，信息增益率\n",
    "    '''\n",
    "    \n",
    "    gain_ratios = dict()\n",
    "    \n",
    "    # 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算\n",
    "    for feature in features:\n",
    "        \n",
    "        # 左子树保证所有的样本的这个特征取值为0\n",
    "        left_split_target = data[data[feature] == 0][target]\n",
    "        \n",
    "        # 右子树保证所有的样本的这个特征取值为1\n",
    "        right_split_target =  data[data[feature] == 1][target]\n",
    "            \n",
    "        # 计算左子树的信息熵\n",
    "        left_entropy = information_entropy(left_split_target)\n",
    "        \n",
    "        # 计算左子树的权重\n",
    "        left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target))\n",
    "\n",
    "        # 计算右子树的信息熵\n",
    "        right_entropy =  information_entropy(right_split_target)                     # YOUR CODE HERE\n",
    "        \n",
    "        # 计算右子树的权重\n",
    "        right_weight =  len(right_split_target) / (len(left_split_target) + len(right_split_target))   # YOUR CODE HERE\n",
    "        \n",
    "        # 计算当前结点的信息熵\n",
    "        current_entropy = information_entropy(data[target])\n",
    "        \n",
    "        # 计算当前结点的信息增益\n",
    "        \n",
    "        gain =  current_entropy - (left_weight * left_entropy + right_weight * right_entropy)                                     # YOUR CODE HERE\n",
    "        \n",
    "        # 计算IV公式中，当前特征为0的值\n",
    "        if left_weight == 0:\n",
    "            left_IV = 0\n",
    "        else:\n",
    "            left_IV = left_weight * np.log2(left_weight)\n",
    "        \n",
    "        # 计算IV公式中，当前特征为1的值\n",
    "        if right_weight == 0:\n",
    "            right_IV = 0\n",
    "        else:\n",
    "            right_IV =  - right_weight * np.log2(right_weight)                             # YOUR CODE HERE\n",
    "        \n",
    "        # IV 等于所有子树IV之和的相反数\n",
    "        IV = - (left_IV + right_IV)\n",
    "            \n",
    "        # 计算使用当前特征划分的信息增益率\n",
    "        # 这里为了防止IV是0，导致除法得到np.inf（无穷），在分母加了一个很小的小数\n",
    "        gain_ratio = gain / (IV + np.finfo(np.longdouble).eps)\n",
    "        \n",
    "        # 信息增益率的存储\n",
    "        gain_ratios[feature] = gain_ratio\n",
    "        \n",
    "        if annotate:\n",
    "            print(\" \", feature, gain_ratio)\n",
    "            \n",
    "    return gain_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08364048435688884\n",
      "-0.02349433005753071\n",
      "-0.0692273287015029\n"
     ]
    }
   ],
   "source": [
    "# 信息增益率测试样例1\n",
    "print(compute_information_gain_ratios(train_data, one_hot_features, target)['grade_A']) # 0.02573\n",
    "\n",
    "# 信息增益率测试样例2\n",
    "print(compute_information_gain_ratios(train_data, one_hot_features, target)['grade_B']) # 0.00417\n",
    "\n",
    "# 信息增益率测试样例3\n",
    "print(compute_information_gain_ratios(train_data, one_hot_features, target)['term_ 60 months']) # 0.01970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 基尼指数\n",
    "数据集$D$的基尼值：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{Gini}(D) & = \\sum^{\\vert \\mathcal{Y} \\vert}_{k=1} \\sum_{k' \\neq k} p_k p_{k'}\\\\\n",
    "& = 1 - \\sum^{\\vert \\mathcal{Y} \\vert}_{k=1} p^2_k.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "属性$a$的基尼指数：\n",
    "\n",
    "$$\n",
    "\\mathrm{Gini\\_index}(D, a) = \\sum^V_{v = 1} \\frac{\\vert D^v \\vert}{\\vert D \\vert} \\mathrm{Gini}(D^v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成数据集基尼值的计算  \n",
    "**这里需要填写三部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(labels_in_node):\n",
    "    '''\n",
    "    计算一个结点内样本的基尼指数\n",
    "    \n",
    "    Paramters\n",
    "    ----------\n",
    "    label_in_data: np.ndarray, 样本的标记，如[-1, -1, 1, 1, 1]\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    gini: float，基尼指数\n",
    "    '''\n",
    "    \n",
    "    # 统计样本总个数\n",
    "    num_of_samples = labels_in_node.shape[0]\n",
    "    \n",
    "    if num_of_samples == 0:\n",
    "        return 0\n",
    "    \n",
    "    # 统计出1的个数\n",
    "    num_of_positive = len(labels_in_node[labels_in_node == 1])\n",
    "    \n",
    "    # 统计出-1的个数\n",
    "    num_of_negative =  num_of_samples - num_of_positive                    # YOUR CODE HERE\n",
    "    \n",
    "    # 统计正例的概率\n",
    "    prob_positive = num_of_positive / num_of_samples\n",
    "    \n",
    "    # 统计负例的概率\n",
    "    prob_negative = num_of_negative / num_of_samples                      # YOUR CODE HERE\n",
    "    \n",
    "    # 计算基尼值\n",
    "    gini = 1 - (prob_positive ** 2 + prob_negative ** 2)                               # YOUR CODE HERE\n",
    "    \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n",
      "0.40816326530612246\n",
      "0.40816326530612246\n",
      "0.4982698961937716\n",
      "0.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 基尼值测试样例1\n",
    "example_labels = np.array([-1, -1, 1, 1, 1])\n",
    "print(gini(example_labels)) # 0.48\n",
    "\n",
    "# 基尼值测试样例2\n",
    "example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])\n",
    "print(gini(example_labels)) # 0.40816\n",
    "    \n",
    "# 基尼值测试样例3\n",
    "example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])\n",
    "print(gini(example_labels)) # 0.40816\n",
    "\n",
    "# 基尼值测试样例4\n",
    "example_labels = np.array([-1] * 9 + [1] * 8)\n",
    "print(gini(example_labels)) # 0.49827\n",
    "\n",
    "# 基尼值测试样例5\n",
    "example_labels = np.array([1] * 8)\n",
    "print(gini(example_labels)) # 0\n",
    "\n",
    "# 基尼值测试样例6\n",
    "example_labels = np.array([])\n",
    "print(gini(example_labels)) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后计算所有特征的基尼指数  \n",
    "**这里需要填写三部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gini_indices(data, features, target, annotate = False):\n",
    "    '''\n",
    "    计算使用各个特征进行划分时，各特征的基尼指数\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    data: pd.DataFrame, 带有特征和标记的数据\n",
    "    \n",
    "    features: list(str)，特征名组成的list\n",
    "    \n",
    "    target: str， 特征的名字\n",
    "    \n",
    "    annotate: boolean, default False，是否打印注释\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    gini_indices: dict, key: str, 特征名\n",
    "                       value: float，基尼指数\n",
    "    '''\n",
    "    \n",
    "    gini_indices = dict()\n",
    "    # 对所有的特征进行遍历，使用当前的划分方法对每个特征进行计算\n",
    "    for feature in features:\n",
    "        # 左子树保证所有的样本的这个特征取值为0\n",
    "        left_split_target = data[data[feature] == 0][target]\n",
    "        \n",
    "        # 右子树保证所有的样本的这个特征取值为1\n",
    "        right_split_target =  data[data[feature] == 1][target]\n",
    "            \n",
    "        # 计算左子树的基尼值\n",
    "        left_gini = gini(left_split_target)\n",
    "        \n",
    "        # 计算左子树的权重\n",
    "        left_weight = len(left_split_target) / (len(left_split_target) + len(right_split_target))\n",
    "\n",
    "        # 计算右子树的基尼值\n",
    "        right_gini =  gini(right_split_target)                                 # YOUR CODE HERE\n",
    "        \n",
    "        # 计算右子树的权重\n",
    "        right_weight = 1 - left_weight                                # YOUR CODE HERE\n",
    "        \n",
    "        # 计算当前结点的基尼指数\n",
    "        gini_index = left_weight * left_gini + right_weight * right_gini                                  # YOUR CODE HERE\n",
    "        \n",
    "        # 存储\n",
    "        gini_indices[feature] = gini_index\n",
    "        \n",
    "        if annotate:\n",
    "            print(\" \", feature, gini_index)\n",
    "            \n",
    "    return gini_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3009520964964362\n",
      "0.3056855375882364\n",
      "0.30055418611740065\n"
     ]
    }
   ],
   "source": [
    "# 基尼指数测试样例1\n",
    "print(compute_gini_indices(train_data, one_hot_features, target)['grade_A']) # 0.30095\n",
    "\n",
    "# 基尼指数测试样例2\n",
    "print(compute_gini_indices(train_data, one_hot_features, target)['grade_B']) # 0.30568\n",
    "\n",
    "# 基尼指数测试样例3\n",
    "print(compute_gini_indices(train_data, one_hot_features, target)['term_ 36 months']) # 0.30055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 完成最优特征的选择 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到此，我们完成了三种划分策略的实现，接下来就是完成获取最优特征的函数  \n",
    "**这里需要填写两个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target, criterion = 'gini', annotate = False):\n",
    "    '''\n",
    "    给定划分方法和数据，找到最优的划分特征\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame, 带有特征和标记的数据\n",
    "    \n",
    "    features: list(str)，特征名组成的list\n",
    "    \n",
    "    target: str， 特征的名字\n",
    "    \n",
    "    criterion: str, 使用哪种指标，三种选项: 'information_gain', 'gain_ratio', 'gini'\n",
    "    \n",
    "    annotate: boolean, default False，是否打印注释\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    best_feature: str, 最佳的划分特征的名字\n",
    "    \n",
    "    '''\n",
    "    if criterion == 'information_gain':\n",
    "        if annotate:\n",
    "            print('using information gain')\n",
    "        \n",
    "        # 得到当前所有特征的信息增益\n",
    "        information_gains = compute_information_gains(data, features, target, annotate)\n",
    "    \n",
    "        # information_gains是一个dict类型的对象，我们要找值最大的那个元素的键是谁\n",
    "        # 根据这些特征和他们的信息增益，找到最佳的划分特征\n",
    "        best_feature = max(information_gains.items(), key = lambda x: x[1])[0]\n",
    "        \n",
    "        return best_feature\n",
    "\n",
    "    elif criterion == 'gain_ratio':\n",
    "        if annotate:\n",
    "            print('using information gain ratio')\n",
    "        \n",
    "        # 得到当前所有特征的信息增益率\n",
    "        gain_ratios = compute_information_gain_ratios(data, features, target, annotate)\n",
    "    \n",
    "        # 根据这些特征和他们的信息增益率，找到最佳的划分特征\n",
    "        best_feature = max(gain_ratios.items(), key=lambda x: x[1])[0]                                                   # YOUR CODE HERE\n",
    "\n",
    "        return best_feature\n",
    "    \n",
    "    elif criterion == 'gini':\n",
    "        if annotate:\n",
    "            print('using gini')\n",
    "        \n",
    "        # 得到当前所有特征的基尼指数\n",
    "        gini_indices = compute_gini_indices(data, features, target, annotate)\n",
    "        \n",
    "        # 根据这些特征和他们的基尼指数，找到最佳的划分特征\n",
    "        best_feature = min(gini_indices.items(), key=lambda x: x[1])[0]                                                   # YOUR CODE HERE\n",
    "\n",
    "        return best_feature\n",
    "    else:\n",
    "        raise Exception(\"传入的criterion不合规!\", criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 判断结点内样本的类别是否为同一类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里需要填写两个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    '''\n",
    "    求树的结点中，样本数少的那个类的样本有多少，比如输入是[1, 1, -1, -1, 1]，返回2\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    labels_in_node: np.ndarray, pd.Series\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    int：个数\n",
    "    \n",
    "    '''\n",
    "    # 如果传入的array为空，返回0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # 统计1的个数\n",
    "    num_of_one =  len(labels_in_node[labels_in_node == 1])                                   # YOUR CODE HERE\n",
    "    \n",
    "    # 统计-1的个数\n",
    "    num_of_minus_one = len(labels_in_node[labels_in_node == -1])                              # YOUR CODE HERE\n",
    "    \n",
    "    return num_of_one if num_of_minus_one > num_of_one else num_of_minus_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 测试样例1\n",
    "print(intermediate_node_num_mistakes(np.array([1, 1, -1, -1, -1]))) # 2\n",
    "\n",
    "# 测试样例2\n",
    "print(intermediate_node_num_mistakes(np.array([]))) # 0\n",
    "\n",
    "# 测试样例3\n",
    "print(intermediate_node_num_mistakes(np.array([1]))) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 创建叶子结点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf(target_values):\n",
    "    '''\n",
    "    计算出当前叶子结点的标记是什么，并且将叶子结点信息保存在一个dict中\n",
    "    \n",
    "    Parameter:\n",
    "    ----------\n",
    "    target_values: pd.Series, 当前叶子结点内样本的标记\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    leaf: dict，表示一个叶结点，\n",
    "            leaf['splitting_features'], None，叶结点不需要划分特征\n",
    "            leaf['left'], None，叶结点没有左子树\n",
    "            leaf['right'], None，叶结点没有右子树\n",
    "            leaf['is_leaf'], True, 是否是叶子结点\n",
    "            leaf['prediction'], int, 表示该叶子结点的预测值\n",
    "    '''\n",
    "    # 创建叶子结点\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True}\n",
    "   \n",
    "    # 数结点内-1和+1的个数\n",
    "    num_ones = len(target_values[target_values == +1])\n",
    "    num_minus_ones = len(target_values[target_values == -1])    \n",
    "\n",
    "    # 叶子结点的标记使用少数服从多数的原则，为样本数多的那类的标记，保存在 leaf['prediction']\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] = 1\n",
    "    else:\n",
    "        leaf['prediction'] = -1\n",
    "\n",
    "    # 返回叶子结点\n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 递归地创建决策树\n",
    "递归的创建决策树  \n",
    "递归算法终止的三个条件：\n",
    "1. 如果结点内所有的样本的标记都相同，该结点就不需要再继续划分，直接做叶子结点即可\n",
    "2. 如果结点所有的特征都已经在之前使用过了，在当前结点无剩余特征可供划分样本，该结点直接做叶子结点\n",
    "3. 如果当前结点的深度已经达到了我们限制的树的最大深度，直接做叶子结点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这里需要填写五个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_create(data, features, target, criterion = 'gini', current_depth = 0, max_depth = 10, annotate = False):\n",
    "    '''\n",
    "    Parameter:\n",
    "    ----------\n",
    "    data: pd.DataFrame, 数据\n",
    "\n",
    "    features: iterable, 特征组成的可迭代对象，比如一个list\n",
    "\n",
    "    target: str, 标记的名字\n",
    "\n",
    "    criterion: 'str', 特征划分方法，只支持三种：'information_gain', 'gain_ratio', 'gini'\n",
    "\n",
    "    current_depth: int, 当前深度，递归的时候需要记录\n",
    "\n",
    "    max_depth: int, 树的最大深度，我们设定的树的最大深度，达到最大深度需要终止递归\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    dict, dict['is_leaf']          : False, 当前顶点不是叶子结点\n",
    "          dict['prediction']       : None, 不是叶子结点就没有预测值\n",
    "          dict['splitting_feature']: splitting_feature, 当前结点是使用哪个特征进行划分的\n",
    "          dict['left']             : dict\n",
    "          dict['right']            : dict\n",
    "    '''\n",
    "    \n",
    "    if criterion not in ['information_gain', 'gain_ratio', 'gini']:\n",
    "        raise Exception(\"传入的criterion不合规!\", criterion)\n",
    "    \n",
    "    # 复制一份特征，存储起来，每使用一个特征进行划分，我们就删除一个\n",
    "    remaining_features = features[:]\n",
    "    \n",
    "    # 取出标记值\n",
    "    target_values = data[target]\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "\n",
    "    # 终止条件1\n",
    "    # 如果当前结点内所有样本同属一类，即这个结点中，各类别样本数最小的那个等于0\n",
    "    # 使用前面写的intermediate_node_num_mistakes来完成这个判断\n",
    "    if intermediate_node_num_mistakes(target_values) == 0: \n",
    "        print(\"Stopping condition 1 reached.\")\n",
    "        return create_leaf(target_values)   # 创建叶子结点\n",
    "    \n",
    "    # 终止条件2\n",
    "    # 如果已经没有剩余的特征可供分割，即remaining_features为空\n",
    "    \n",
    "    if  len(remaining_features) == 0:                                                                # YOUR CODE HERE\n",
    "        print(\"Stopping condition 2 reached.\")\n",
    "        return create_leaf(target_values)   # 创建叶子结点\n",
    "    \n",
    "    # 终止条件3\n",
    "    # 如果已经到达了我们要求的最大深度，即当前深度达到了最大深度\n",
    "    \n",
    "    if  current_depth >= max_depth:                                                                # YOUR CODE HERE\n",
    "        print(\"Reached maximum depth. Stopping for now.\")\n",
    "        return create_leaf(target_values)   # 创建叶子结点\n",
    "\n",
    "    # 找到最优划分特征\n",
    "    # 使用best_splitting_feature这个函数\n",
    "    \n",
    "    splitting_feature = best_splitting_feature(data,features,target,criterion)\n",
    "    \n",
    "    # 使用我们找到的最优特征将数据划分成两份\n",
    "    # 左子树的数据\n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    \n",
    "    # 右子树的数据\n",
    "    right_split = data[data[splitting_feature] == 1]                                                    # YOUR CODE HERE\n",
    "    \n",
    "    # 现在已经完成划分，我们要从剩余特征中删除掉当前这个特征\n",
    "    remaining_features.remove(splitting_feature)\n",
    "    \n",
    "    # 打印当前划分使用的特征，打印左子树样本个数，右子树样本个数\n",
    "    print(\"Split on feature %s. (%s, %s)\" % (\\\n",
    "                      splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # 如果使用当前的特征，将所有的样本都划分到一棵子树中，那么就直接将这棵子树变成叶子结点\n",
    "    # 判断左子树是不是“完美”的\n",
    "    if len(left_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target])\n",
    "    \n",
    "    # 判断右子树是不是“完美”的\n",
    "    if len(right_split) == len(data):\n",
    "        print(\"Creating right node.\")\n",
    "        return create_leaf(right_split[target])                                                      # YOUR CODE HERE\n",
    "\n",
    "    # 递归地创建左子树\n",
    "    left_tree = decision_tree_create(left_split, remaining_features, target, criterion, current_depth + 1, max_depth, annotate)\n",
    "    \n",
    "    # 递归地创建右子树\n",
    "    \n",
    "    right_tree =  decision_tree_create(right_split, remaining_features, target, criterion, current_depth + 1, max_depth, annotate)                                                  # YOUR CODE HERE\n",
    "\n",
    "    # 返回树的非叶子结点\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Subtree, depth = 0 (73564 data points).\n",
      "Split on feature term_ 36 months. (14831, 58733)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (14831 data points).\n",
      "Split on feature grade_F. (13003, 1828)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (13003 data points).\n",
      "Split on feature grade_E. (9818, 3185)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (9818 data points).\n",
      "Split on feature home_ownership_RENT. (6796, 3022)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (6796 data points).\n",
      "Split on feature grade_G. (6507, 289)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (6507 data points).\n",
      "Split on feature grade_D. (4368, 2139)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4368 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2139 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (289 data points).\n",
      "Split on feature home_ownership_OWN. (249, 40)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (249 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (40 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (3022 data points).\n",
      "Split on feature grade_G. (2827, 195)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2827 data points).\n",
      "Split on feature grade_D. (1651, 1176)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1651 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1176 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (195 data points).\n",
      "Split on feature emp_length_2 years. (176, 19)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (176 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (19 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (3185 data points).\n",
      "Split on feature home_ownership_RENT. (1980, 1205)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (1980 data points).\n",
      "Split on feature emp_length_3 years. (1828, 152)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (1828 data points).\n",
      "Split on feature emp_length_10+ years. (1057, 771)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1057 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (771 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (152 data points).\n",
      "Split on feature home_ownership_OTHER. (151, 1)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (151 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (1205 data points).\n",
      "Split on feature emp_length_1 year. (1124, 81)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (1124 data points).\n",
      "Split on feature emp_length_8 years. (1073, 51)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1073 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (51 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (81 data points).\n",
      "Split on feature grade_A. (81, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (1828 data points).\n",
      "Split on feature home_ownership_RENT. (1030, 798)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (1030 data points).\n",
      "Split on feature emp_length_3 years. (957, 73)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (957 data points).\n",
      "Split on feature emp_length_2 years. (886, 71)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (886 data points).\n",
      "Split on feature home_ownership_OTHER. (884, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (884 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (71 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (12, 59)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (12 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (59 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (73 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (13, 60)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (13 data points).\n",
      "Split on feature grade_A. (13, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (60 data points).\n",
      "Split on feature grade_A. (60, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (798 data points).\n",
      "Split on feature emp_length_7 years. (740, 58)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (740 data points).\n",
      "Split on feature emp_length_3 years. (673, 67)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (673 data points).\n",
      "Split on feature emp_length_9 years. (646, 27)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (646 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (27 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (67 data points).\n",
      "Split on feature grade_A. (67, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (58 data points).\n",
      "Split on feature grade_A. (58, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (58733 data points).\n",
      "Split on feature grade_A. (45632, 13101)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (45632 data points).\n",
      "Split on feature grade_B. (25130, 20502)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (25130 data points).\n",
      "Split on feature grade_C. (11066, 14064)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (11066 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (6987, 4079)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (6987 data points).\n",
      "Split on feature grade_F. (6650, 337)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (6650 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (337 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (4079 data points).\n",
      "Split on feature grade_G. (4003, 76)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4003 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (76 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (14064 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (8209, 5855)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (8209 data points).\n",
      "Split on feature emp_length_2 years. (7209, 1000)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (7209 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1000 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (5855 data points).\n",
      "Split on feature emp_length_10+ years. (3802, 2053)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (3802 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2053 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (20502 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (10775, 9727)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (10775 data points).\n",
      "Split on feature home_ownership_OTHER. (10741, 34)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (10741 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature emp_length_1 year. (9754, 987)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (9754 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (987 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (34 data points).\n",
      "Split on feature emp_length_10+ years. (27, 7)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (27 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (7 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (9727 data points).\n",
      "Split on feature emp_length_< 1 year. (9186, 541)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (9186 data points).\n",
      "Split on feature emp_length_9 years. (8807, 379)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (8807 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (379 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (541 data points).\n",
      "Split on feature grade_C. (541, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (13101 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (5830, 7271)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (5830 data points).\n",
      "Split on feature emp_length_3 years. (5283, 547)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (5283 data points).\n",
      "Split on feature emp_length_1 year. (4705, 578)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (4705 data points).\n",
      "Split on feature emp_length_7 years. (4467, 238)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4467 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (238 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (578 data points).\n",
      "Split on feature home_ownership_OTHER. (576, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (576 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (547 data points).\n",
      "Split on feature home_ownership_OTHER. (545, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (545 data points).\n",
      "Split on feature home_ownership_OWN. (468, 77)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (468 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (77 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2 data points).\n",
      "Split on feature grade_B. (2, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (7271 data points).\n",
      "Split on feature emp_length_2 years. (6702, 569)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (6702 data points).\n",
      "Split on feature emp_length_4 years. (6234, 468)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (6234 data points).\n",
      "Split on feature emp_length_3 years. (5689, 545)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (5689 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (545 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (468 data points).\n",
      "Split on feature grade_B. (468, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (569 data points).\n",
      "Split on feature grade_B. (569, 0)\n",
      "Creating leaf node.\n"
     ]
    }
   ],
   "source": [
    "my_decision_tree = decision_tree_create(train_data, one_hot_features, target, 'gini', max_depth = 6, annotate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，模型就训练好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们需要完成预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):\n",
    "    '''\n",
    "    递归的进行预测，一次只能预测一个样本\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dict\n",
    "    \n",
    "    x: pd.Series，待预测的样本\n",
    "    \n",
    "    annotate： boolean, 是否显示注释\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    返回预测的标记\n",
    "    '''\n",
    "    if tree['is_leaf']:\n",
    "        if annotate:\n",
    "            print (\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction']\n",
    "    else:\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate:\n",
    "             print (\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们取测试集第一个样本来测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe_loans                     1\n",
      "grade_A                    False\n",
      "grade_B                    False\n",
      "grade_C                    False\n",
      "grade_D                    False\n",
      "grade_E                     True\n",
      "grade_F                    False\n",
      "grade_G                    False\n",
      "term_ 36 months             True\n",
      "term_ 60 months            False\n",
      "home_ownership_MORTGAGE     True\n",
      "home_ownership_OTHER       False\n",
      "home_ownership_OWN         False\n",
      "home_ownership_RENT        False\n",
      "emp_length_1 year          False\n",
      "emp_length_10+ years       False\n",
      "emp_length_2 years          True\n",
      "emp_length_3 years         False\n",
      "emp_length_4 years         False\n",
      "emp_length_5 years         False\n",
      "emp_length_6 years         False\n",
      "emp_length_7 years         False\n",
      "emp_length_8 years         False\n",
      "emp_length_9 years         False\n",
      "emp_length_< 1 year        False\n",
      "Name: 37225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_sample = test_data.iloc[0]\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True class: 1 \n",
      "Predicted class: 1 \n"
     ]
    }
   ],
   "source": [
    "print('True class: %s ' % (test_sample['safe_loans']))\n",
    "print('Predicted class: %s ' % classify(my_decision_tree, test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印出使用决策树判断的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on term_ 36 months = True\n",
      "Split on grade_A = False\n",
      "Split on grade_B = False\n",
      "Split on grade_C = False\n",
      "Split on home_ownership_MORTGAGE = True\n",
      "Split on grade_G = False\n",
      "At leaf, predicting 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(my_decision_tree, test_sample, annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 在测试集上对我们的模型进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先来编写一个批量预测的函数，传入的是整个测试集那样的pd.DataFrame，这个函数返回一个np.ndarray，存储模型的预测结果  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, data):\n",
    "    '''\n",
    "    按行遍历data，对每个样本进行预测，将值存在prediction中，最后返回np.ndarray\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    tree: dict, 模型\n",
    "    \n",
    "    data: pd.DataFrame, 数据\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    predictions：np.ndarray, 模型对这些样本的预测结果\n",
    "    '''\n",
    "    predictions = np.zeros(len(data)) # 长度和data一样\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        predictions[i] = classify(tree,data.iloc[i])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 请你计算使用不同评价指标得到模型的四项指标的值，填写在下方表格内\n",
    "**树的最大深度为6**  \n",
    "**这里需要填写一个部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Subtree, depth = 0 (73564 data points).\n",
      "Split on feature term_ 36 months. (14831, 58733)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (14831 data points).\n",
      "Split on feature grade_F. (13003, 1828)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (13003 data points).\n",
      "Split on feature grade_E. (9818, 3185)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (9818 data points).\n",
      "Split on feature home_ownership_RENT. (6796, 3022)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (6796 data points).\n",
      "Split on feature grade_G. (6507, 289)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (6507 data points).\n",
      "Split on feature grade_D. (4368, 2139)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4368 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2139 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (289 data points).\n",
      "Split on feature home_ownership_OWN. (249, 40)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (249 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (40 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (3022 data points).\n",
      "Split on feature grade_G. (2827, 195)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2827 data points).\n",
      "Split on feature grade_D. (1651, 1176)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1651 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1176 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (195 data points).\n",
      "Split on feature emp_length_2 years. (176, 19)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (176 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (19 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (3185 data points).\n",
      "Split on feature home_ownership_RENT. (1980, 1205)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (1980 data points).\n",
      "Split on feature emp_length_3 years. (1828, 152)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (1828 data points).\n",
      "Split on feature emp_length_10+ years. (1057, 771)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1057 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (771 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (152 data points).\n",
      "Split on feature home_ownership_OTHER. (151, 1)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (151 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (1205 data points).\n",
      "Split on feature emp_length_1 year. (1124, 81)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (1124 data points).\n",
      "Split on feature emp_length_8 years. (1073, 51)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1073 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (51 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (81 data points).\n",
      "Split on feature grade_A. (81, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (1828 data points).\n",
      "Split on feature home_ownership_RENT. (1030, 798)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (1030 data points).\n",
      "Split on feature emp_length_3 years. (957, 73)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (957 data points).\n",
      "Split on feature emp_length_2 years. (886, 71)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (886 data points).\n",
      "Split on feature home_ownership_OTHER. (884, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (884 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (71 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (12, 59)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (12 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (59 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (73 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (13, 60)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (13 data points).\n",
      "Split on feature grade_A. (13, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (60 data points).\n",
      "Split on feature grade_A. (60, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (798 data points).\n",
      "Split on feature emp_length_7 years. (740, 58)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (740 data points).\n",
      "Split on feature emp_length_3 years. (673, 67)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (673 data points).\n",
      "Split on feature emp_length_9 years. (646, 27)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (646 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (27 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (67 data points).\n",
      "Split on feature grade_A. (67, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (58 data points).\n",
      "Split on feature grade_A. (58, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (58733 data points).\n",
      "Split on feature grade_A. (45632, 13101)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (45632 data points).\n",
      "Split on feature grade_B. (25130, 20502)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (25130 data points).\n",
      "Split on feature grade_C. (11066, 14064)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (11066 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (6987, 4079)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (6987 data points).\n",
      "Split on feature grade_F. (6650, 337)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (6650 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (337 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (4079 data points).\n",
      "Split on feature grade_G. (4003, 76)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4003 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (76 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (14064 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (8209, 5855)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (8209 data points).\n",
      "Split on feature emp_length_2 years. (7209, 1000)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (7209 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1000 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (5855 data points).\n",
      "Split on feature emp_length_10+ years. (3802, 2053)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (3802 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2053 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (20502 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (10775, 9727)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (10775 data points).\n",
      "Split on feature home_ownership_OTHER. (10741, 34)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (10741 data points).\n",
      "Split on feature emp_length_1 year. (9754, 987)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (9754 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (987 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (34 data points).\n",
      "Split on feature emp_length_10+ years. (27, 7)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (27 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (7 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (9727 data points).\n",
      "Split on feature emp_length_< 1 year. (9186, 541)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (9186 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature emp_length_9 years. (8807, 379)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (8807 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (379 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (541 data points).\n",
      "Split on feature grade_C. (541, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (13101 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (5830, 7271)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (5830 data points).\n",
      "Split on feature emp_length_3 years. (5283, 547)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (5283 data points).\n",
      "Split on feature emp_length_1 year. (4705, 578)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (4705 data points).\n",
      "Split on feature emp_length_7 years. (4467, 238)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4467 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (238 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (578 data points).\n",
      "Split on feature home_ownership_OTHER. (576, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (576 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (547 data points).\n",
      "Split on feature home_ownership_OTHER. (545, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (545 data points).\n",
      "Split on feature home_ownership_OWN. (468, 77)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (468 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (77 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2 data points).\n",
      "Split on feature grade_B. (2, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (7271 data points).\n",
      "Split on feature emp_length_2 years. (6702, 569)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (6702 data points).\n",
      "Split on feature emp_length_4 years. (6234, 468)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (6234 data points).\n",
      "Split on feature emp_length_3 years. (5689, 545)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (5689 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (545 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (468 data points).\n",
      "Split on feature grade_B. (468, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (569 data points).\n",
      "Split on feature grade_B. (569, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 0 (73564 data points).\n",
      "Split on feature grade_A. (60204, 13360)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (60204 data points).\n",
      "Split on feature grade_B. (37768, 22436)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (37768 data points).\n",
      "Split on feature grade_C. (19878, 17890)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (19878 data points).\n",
      "Split on feature term_ 36 months. (8812, 11066)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (8812 data points).\n",
      "Split on feature home_ownership_RENT. (5438, 3374)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (5438 data points).\n",
      "Split on feature grade_D. (3299, 2139)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (3299 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2139 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (3374 data points).\n",
      "Split on feature grade_D. (2198, 1176)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2198 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1176 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (11066 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (6987, 4079)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (6987 data points).\n",
      "Split on feature grade_F. (6650, 337)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (6650 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (337 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (4079 data points).\n",
      "Split on feature grade_G. (4003, 76)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4003 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (76 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (17890 data points).\n",
      "Split on feature term_ 36 months. (3826, 14064)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (3826 data points).\n",
      "Split on feature home_ownership_RENT. (2750, 1076)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2750 data points).\n",
      "Split on feature emp_length_1 year. (2616, 134)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2616 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (134 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (1076 data points).\n",
      "Split on feature emp_length_6 years. (1018, 58)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1018 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (58 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (14064 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (8209, 5855)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (8209 data points).\n",
      "Split on feature emp_length_2 years. (7209, 1000)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (7209 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1000 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (5855 data points).\n",
      "Split on feature emp_length_10+ years. (3802, 2053)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (3802 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2053 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (22436 data points).\n",
      "Split on feature term_ 36 months. (1934, 20502)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (1934 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (689, 1245)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (689 data points).\n",
      "Split on feature emp_length_7 years. (647, 42)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (647 data points).\n",
      "Split on feature emp_length_10+ years. (483, 164)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (483 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (164 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (42 data points).\n",
      "Split on feature home_ownership_OWN. (34, 8)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (34 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (8 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (1245 data points).\n",
      "Split on feature emp_length_3 years. (1152, 93)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (1152 data points).\n",
      "Split on feature emp_length_7 years. (1081, 71)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (71 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (93 data points).\n",
      "Split on feature grade_C. (93, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (20502 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature home_ownership_MORTGAGE. (10775, 9727)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (10775 data points).\n",
      "Split on feature emp_length_1 year. (9785, 990)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (9785 data points).\n",
      "Split on feature home_ownership_OTHER. (9754, 31)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (9754 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (31 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (990 data points).\n",
      "Split on feature home_ownership_OTHER. (987, 3)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (987 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (3 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (9727 data points).\n",
      "Split on feature emp_length_< 1 year. (9186, 541)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (9186 data points).\n",
      "Split on feature emp_length_9 years. (8807, 379)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (8807 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (379 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (541 data points).\n",
      "Split on feature grade_C. (541, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (13360 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (5900, 7460)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (5900 data points).\n",
      "Split on feature term_ 36 months. (70, 5830)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (70 data points).\n",
      "Split on feature home_ownership_OWN. (50, 20)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (50 data points).\n",
      "Split on feature emp_length_4 years. (48, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (48 data points).\n",
      "Split on feature emp_length_7 years. (47, 1)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (47 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2 data points).\n",
      "Split on feature grade_B. (2, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (20 data points).\n",
      "Split on feature emp_length_5 years. (17, 3)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (17 data points).\n",
      "Split on feature emp_length_1 year. (15, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (15 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (3 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (5830 data points).\n",
      "Split on feature emp_length_3 years. (5283, 547)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (5283 data points).\n",
      "Split on feature emp_length_1 year. (4705, 578)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (4705 data points).\n",
      "Split on feature home_ownership_OTHER. (4689, 16)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (4689 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (16 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (578 data points).\n",
      "Split on feature home_ownership_OTHER. (576, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (576 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (547 data points).\n",
      "Split on feature home_ownership_OTHER. (545, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (545 data points).\n",
      "Split on feature home_ownership_OWN. (468, 77)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (468 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (77 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2 data points).\n",
      "Split on feature grade_B. (2, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (7460 data points).\n",
      "Split on feature emp_length_2 years. (6879, 581)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (6879 data points).\n",
      "Split on feature emp_length_4 years. (6397, 482)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (6397 data points).\n",
      "Split on feature emp_length_1 year. (6036, 361)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (6036 data points).\n",
      "Split on feature emp_length_3 years. (5475, 561)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (5475 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (561 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (361 data points).\n",
      "Split on feature term_ 36 months. (7, 354)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (7 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (354 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (482 data points).\n",
      "Split on feature term_ 36 months. (14, 468)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (14 data points).\n",
      "Split on feature grade_B. (14, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (468 data points).\n",
      "Split on feature grade_B. (468, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (581 data points).\n",
      "Split on feature term_ 36 months. (12, 569)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (12 data points).\n",
      "Split on feature grade_B. (12, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (569 data points).\n",
      "Split on feature grade_B. (569, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 0 (73564 data points).\n",
      "Split on feature term_ 36 months. (14831, 58733)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (14831 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (6195, 8636)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (6195 data points).\n",
      "Split on feature home_ownership_RENT. (1170, 5025)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (1170 data points).\n",
      "Split on feature home_ownership_OWN. (4, 1166)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (4 data points).\n",
      "Split on feature grade_F. (2, 2)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2 data points).\n",
      "Split on feature grade_E. (1, 1)\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 6 (1 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 5 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 4 (1166 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature term_ 60 months. (0, 1166)\n",
      "Creating right node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 3 (5025 data points).\n",
      "Split on feature term_ 60 months. (0, 5025)\n",
      "Creating right node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 2 (8636 data points).\n",
      "Split on feature term_ 60 months. (0, 8636)\n",
      "Creating right node.\n",
      "--------------------------------------------------\n",
      "Subtree, depth = 1 (58733 data points).\n",
      "Split on feature term_ 60 months. (58733, 0)\n",
      "Creating leaf node.\n",
      "Evaluation results for decision tree using 'gini' criterion:\n",
      "Accuracy: 0.8117366392757376\n",
      "Precision: 0.8128399100388468\n",
      "Recall: 0.9980168193799422\n",
      "F1-Score: 0.8959603357935657\n",
      "\n",
      "Evaluation results for decision tree using 'information_gain' criterion:\n",
      "Accuracy: 0.8122056154802928\n",
      "Precision: 0.8122387390142941\n",
      "Recall: 0.9999497928956947\n",
      "F1-Score: 0.8963724740087312\n",
      "\n",
      "Evaluation results for decision tree using 'gain_ratio' criterion:\n",
      "Accuracy: 0.8122463960198193\n",
      "Precision: 0.8122463960198193\n",
      "Recall: 1.0\n",
      "F1-Score: 0.8963973086703121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_decision_tree_list = ['gini','information_gain','gain_ratio']\n",
    "\n",
    "\n",
    "# 创建字典来存储不同划分标准下的四项指标值\n",
    "evaluation_results = {}\n",
    "\n",
    "for term in my_decision_tree_list:\n",
    "    # 使用 term 作为划分标准创建决策树\n",
    "    my_decision_tree = decision_tree_create(train_data, one_hot_features, target, term, max_depth=6, annotate=False)\n",
    "\n",
    "    # 预测测试集\n",
    "    predictions = predict(my_decision_tree, test_data)\n",
    "\n",
    "    # 计算四项指标\n",
    "    accuracy = accuracy_score(test_data[target], predictions)\n",
    "    precision = precision_score(test_data[target], predictions, average='binary')\n",
    "    recall = recall_score(test_data[target], predictions, average='binary')\n",
    "    f1 = f1_score(test_data[target], predictions, average='binary')\n",
    "\n",
    "    # 存储四项指标值\n",
    "    evaluation_results[term] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}\n",
    "\n",
    "# 打印结果\n",
    "for term, metrics in evaluation_results.items():\n",
    "    print(f\"Evaluation results for decision tree using '{term}' criterion:\")\n",
    "    print(\"Accuracy:\", metrics['Accuracy'])\n",
    "    print(\"Precision:\", metrics['Precision'])\n",
    "    print(\"Recall:\", metrics['Recall'])\n",
    "    print(\"F1-Score:\", metrics['F1-Score'])\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "树的最大深度为6  \n",
    "\n",
    "###### 双击此处编写\n",
    "\n",
    "划分标准|精度|查准率|查全率|F1\n",
    "-|-|-|-|-\n",
    "信息增益|0.8122463960198193|0.8122463960198193|1.0|0.8963973086703121\n",
    "信息增益率|0.8122056154802928|0.8122387390142941|0.9999497928956947|0.8963724740087312\n",
    "基尼指数|0.8117366392757376|0.8128399100388468|0.9980168193799422|0.8959603357935657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
